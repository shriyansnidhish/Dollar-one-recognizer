Human-Centered Input Recognition Algorithms - Project 2 Final 

Group members:-
- Shriyans Nidhish
- Smridhi Bhat

Submission info:-
- This submission contains readme.txt, Project2Final.zip, Project2.pptx
- Upon unzipping Project2Final.zip there are following folders and files:- 
- HeatMaps:- This folder contains 3 heatmaps images and 3 feature-data for all 3 modalities.
- Dataset :- This folder contains the the gesture XML files for all 3 modalities.
- logFiles :- This folder contains 6 csv log files. 2 for each modality (10 iteration and 100 iteration).
- Consent-forms :- This folder contains the participants consent forms.
- graph.png :- This is the graph (accuracy vs no. of iterations) image generated for each modality.
- demo1.mp4 :- This is a short video demo demonstrating data collection app and XML file generation.
- demo2.mp4 :- This is a short video demo demonstrating offline recognizer and logFile generation.


Goals achieved:
a) Collected data from 6 user using 3 different modalities.
- As per our project1 extension, we collected gesture data from 6 users through 3 different modalities (Mousepad, TouchScreen without stylus, TouchScreen with stylus).
- The data was collected using the app developed in Project1part4.
- All the users consent was taken by getting the participant-consent forms signed.
- No code changes were done.


b) Performed offline recognition and generated logFile.
- Offline recognition was performed on the data collected in (a). 
- It was performed for 6 users and 10 iterations as well as 6 users and 100 iterations for each modality.
- Total of 6 logFiles are included in the zip submission.


c) Visualized data through the GHOST heatmap toolkit.
- After running the data through GHOST heat map toolkit, 3 heatmap images and 3 feature-data.csv was exported for each modality for further analysis.


d) Predicted outcome matched with Actual outcome.
- We predicted that the 1$ recognition algorithm may perform poorly with certain data collection mediums, such as touchscreen with stylus, while performing better with MousePad and actual outcome showed that the accuracy of data collected using MousePad was significantly higher than that of data collected using TouchScreen with stylus.

Note :- There were no code changes performed in Project2 execution and hence, source code is not  included in the submission.

